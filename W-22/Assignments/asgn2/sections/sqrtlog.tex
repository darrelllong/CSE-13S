\section{Square Roots and Logarithms}\label{section:sqrtlog}

Can we just use the Taylor series to compute square root and the natural
logarithm? Consider that $\sqrt{x} = x^\frac{1}{2}$, and so it is
already a series but it has just \emph{one term}. If we attempt a Taylor series we see that
$$
\frac{d}{dx} \sqrt{x} = \frac{1}{2 \sqrt{x}}
\quad\text{and}\quad \frac{d^2}{dx^2} \sqrt{x} = -\frac{1}{4 x^{3/2}}
\quad\text{and}\quad \frac{d^3}{dx^3} \sqrt{x} = \frac{3}{8 x^{5/2}} \quad\ldots
$$
all of which contain $\sqrt{x}$, thus doing us no good at all.
How about a series for
$\log(x)$? Since $\log(0)$ is undefined, we will use $\log(x+1)$:
$$
\log(x+1) =
x-\frac{x^2}{2}+\frac{x^3}{3}-\frac{x^4}{4}+\frac{x^5}{5}-\frac{x^6}{6}+\frac{x^7}{7}+O\left(x^8\right).
$$
It exists, but it converges \emph{extremely slowly}.

To compute $\sqrt{x}$ and $\log(x)$, you will use Newton's method,
also called the Newton-Raphson method. It is an iterative algorithm
to approximate roots of real-valued functions:
solving $f(x) = 0$. Each iteration $k+1$ of Newton's method produces
successively better approximation:
\[
  x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)} .
\]

For example, consider computing $\sqrt{y}$  with Newton's method. That
is, in order to solve for some $\sqrt{y}$, we are searching for a \emph{non-negative} $x$
such that $x^2 = y$. We can express this as finding the root of $f(x) = x^2 -
y$, giving us:
\[
  x_{k+1} = x_k - \frac{x_k^2-y}{2x_k} = \frac{y}{2 x_k}+\frac{x_k}{2} = \frac{1}{2}\left( x_k + \frac{y}{x_k} \right).
\]
Each guess $x_{k+1}$ gives a successive improvement over the previous guess
$x_k$. Your function \texttt{Sqrt()} should behave the same as \texttt{log()}
from \texttt{<math.h>}: compute $\sqrt{x}$. The following is an example that
implements Newton's method for computing square roots.

\begin{pylisting}{Implementation of $\sqrt{x}$}
def sqrt(x, epsilon = 1e-14):
    z = 0.0
    y = 1.0
    while abs(y - z) > epsilon:
        z = y
        y = 0.5 * (z + x / z)
    return y
\end{pylisting}

Your function \texttt{Log()} should behave the same as \texttt{log()} from
\texttt{<math.h>}: compute $\log(x)$ ($\ln(x)$). The procedure is very much the
same as it was for the square root example, the main difference being that $f(x)
= y - e^x$, since $e^x$ is the inverse of $\log$, \textit{i.e.} $\log(e^x) = x$.
Another key difference is the value converges when $e^{x_{i}} - y$ is small,
where $x_0$ is initially $1.0$ and is used to compute better approximations. In
order to implement this function, you will have to use your \texttt{Exp()}
function.

\begin{pylisting}{Implementation of $\log(x)$}
def log(x, epsilon = 1e-14):
    y = 1.0
    p = exp(y)
    while abs(p - x) > epsilon:
        y = y + x / p - 1
        p = exp(y)
    return y
\end{pylisting}

\subsection{Scaling}
You can implement the $\log(x)$ and $\sqrt{x}$ functions directly, and if you do
you will find that they work well for small $x$ but fail when $x$ increases. For
$\sqrt{x}$ the algorithm is simply less efficient, but for $\log(x)$ it will
fail for $x> 29$ due to floating-point numbers not being real numbers. What
should we do? The answer is surprisingly simple: we scale the problem to a small
interval.

In the case of $\sqrt{x}$ we will factor out all powers of \emph{four}:
let $x = 4^k \times a $. This is trivially true for $k=0$, and you will
see that it can be done for $k\ge 1$ if $x\ge 4$. Suppose $x=48$, then
$x = 4^2\times 3$, with $a=3$. Observe that $\sqrt{4^k a} =
2^k\sqrt{a}$. For every $4$ that we factor out, we multiply by $2$
after we have computed the $\sqrt{a}$. In our example,
$\sqrt{48} = \sqrt{4^2\times 2} = 2^2\sqrt{3}= 4\sqrt{3}$. Why $4$? Four is a
\emph{perfect square}, so it is easier.

\begin{pylisting}{}
f = 1.0 # 2^0
while x > 1:
    x /= 4.0
    f *= 2.0 # 2^(k+1)
\end{pylisting}

In the case of $\log(x)$ observe that $\log(x) = \log(a\times e^f
) = \log(a) + \log(e^f)  = f + \log(a)$ when $x = e^f\times a$. We factor out $e$, and
each time add $1$ to $f$ until $a<e$. This allows us to search for
$\log(a)$ over the small interval $[1, e)$.  For example, to calculate
$\log(917) \approx \log(e^7 \times 0.836195762413492) \approx 7 +
\log(0.836195762413492) \approx 6.821107472256466$.

\begin{pylisting}{}
e = 2.7182818284590455 # Euler's constant
while x > e:
    x /= e
    f += 1.0
\end{pylisting}
