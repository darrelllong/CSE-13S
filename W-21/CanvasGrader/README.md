# Canvas Grader

Contains three scripts:

- ```roster.py``` - Handles dumping the Canvas course roster into a CSV containing each student's name, CanvasID, CruzID, and GitLab repo.

- ```cloner.py``` - Handles parsing Canvas submitted commit IDs for a specified assignment for all students in a specified CSV (must be laid out like the one generated by ```roster.py```), cloning their individual repos, and checking out their repos to the parsed commit ID. Also creates score files for each student with a submission corresponding to the specified plaintext rubric for the assignment.

- ```scorer.py``` - Handles parsing filled-out plaintext rubric files for all students in a specified CSV (just like with ```cloner.py```), then uploading the parsed scores and comments to Canvas.



## Requirements

1. You need to have a personal Canvas access token.

   You can generate a Canvas access token through the Canvas site under Account >> Settings >> Approved Integrations >> New Access Token. Make sure to note the generated token somewhere since Canvas does not let you view the token after closing the initial modal.

2. Secondly, you need an installation of ```python3``` that is recent enough in order to install the packages found in ```requirements.txt```. To install these packages:

```
pip3 install -r requirements.txt
```



## Setup

You will need alter ```config.json``` to your needs. There are 3 names/values that appear in this config file:

```json
{
  "canvas_url": "https://canvas.ucsc.edu",
  "course_id": 35728,
  "token": "<your Canvas access token>"
}
```

The value of ```canvas_url``` shouldn't need to be changed for any UCSC-hosted course.

The ```course_id``` seen above is for the Fall 2020 offering of CSE 13S at UCSC; you can find any course ID directly from the course page's url. This value is expected to be an integer.

The ```token``` value is for your Canvas access token as a string.

You may find this helpful:

```
$ git update-index --assume-unchanged config.json
```

Since everyone's ```config.json``` will differ from each other's, this updates the ```git``` index of the config file so it will assume it is always unchanged, hiding it as a modified file when ```git status``` is run.



## Usage

**Getting the course roster:**

You can get the course roster by running the ```roster.py``` script:

```
$ python3 roster.py [-h] [-o [OUTFILE]]
```

This script, as with all the other scripts, allows additional command-line flags such as ```-h``` for help/script usage. The only flag of note for this script is ```-o``` to specify the file path of the generated CSV which is ```students.csv``` by default.



**Cloning the repos for a specific assignment:**

You will most likely want to clone the repos for only a subset of the entire student roster for grading. It is recommended to take the CSV generated through ```roster.py``` and removing the rows for all the students you aren't grading, leaving only the students in your grading bucket.

This script requires an assignment number and CSV to be specified via command-line arguments:

```
$ python3 cloner.py [-h] -a [ASSIGNMENT] -c [CSV]
```

Both the assignment number and CSV are required arguments. An example command that you would run to clone all the repos for students found in ```grading-bucket.csv``` and checkout their code to the commit ID they submitted for assignment 1:

```
$ python3 cloner.py -a 1 -c grading-bucket.csv
```

Cloned repos will be found under the generated ```repos``` directory.



**Grading students and uploading their scored rubrics:**

The inspiration for the hopefully streamlined grading is taken from Wesley Mackey, UCSC's most mysterious professor (just try Googling for him). Each assignment is given a plaintext rubric file. Each student you intend to grade receives a copy of this rubric file, uniquely identified by CruzID, which is generated automatically after running ```cloner.py```.

As an example, assume you are grading assignment 1. After running ```cloner.py``` with the CSV ```grading-bucket.csv``` a directory named ```scores/asgn1``` is created automatically with a plaintext rubric/score file for each student in ```grading-bucket.csv``` with a submission. Say you're grading the student Sammy Slug who has the CruzID ```sammyslug```. In ```scores/asgn1/sammyslug.score``` you should see instructions for grading each section. An example section would be for program functionality:

```
==================================================================

FUNCTIONALITY (60 points max)

  (60) Subtract 5 points for each test with no output.
       Subtract 3 points for each test with incorrect output.

functionality score: x/60

functionality comments:
<<>>

<<>>

END FUNCTIONALITY CRITERION

==================================================================

```

To fill out this section, simply change the ```x``` found in the line ```functionality score: x/60``` to whatever it is the student received for this grading criterion. Any comments you intend for the student should go between the double angle brackets; don't worry, it's capable of multi-line comments. An example filled out section:

```
==================================================================

FUNCTIONALITY (60 points max)

  (60) Subtract 5 points for each test with no output.
       Subtract 3 points for each test with incorrect output.

functionality score: 50/60

functionality comments:
<<>>
Failed two tests:
 - test1
 - test2
<<>>

END FUNCTIONALITY CRITERION

==================================================================

```

It is also fine to not leave a comment, in which case you should simply leave the linebreak separating the double angle brackets. Also make sure that the score is formatted as ```x/y``` with no extraneous spaces.

With the ```scorer.py``` script, you can upload graded rubrics in bulk. Simply have a scored rubric file for each student in your grading bucket under a directory named ```scores``` and make sure, like with ```cloner.py```, have a properly-formatted CSV with all the students you wish to upload graded rubrics for.

```
$ python3 scorer.py [-h] -a [ASSIGNMENT] -c [CSV]
```

Similar to ```cloner.py```, ```scorer.py``` requires you to specify the assignment the rubrics are meant for, as well as the CSV of your grading bucket.



## Future Work

Secondary scripts that integrate assignment-specific grading scripts will be integrated in the near future.
