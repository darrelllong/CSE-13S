\section{$k$-Nearest Neighbors}

Commonly used in machine learning, the $k$-nearest neighbors (KNN) algorithm is
typically used for either classification or regression of certain objects. In
our case, we wish to classify samples of texts and determine which authors are
most likely to have authored an anonymous sample of text.

Assume you have $n$ samples of text and 1 anonymous sample of text to identify.
To identify the top $k$ likely authors of the sample text, we compute the
distance from this anonymous sample to each of the $n$ known samples using their
respective \emph{feature vectors}. A feature vector in this case is simply the
count of various words in the sample of text. This means that we effectively
have an $n$-dimensional vector, where $n$ is the number of unique words in some
text.

The basic algorithm to identify likely candidates as authors of some anonymous
text is as follows: First, you must first read a set of sample texts from known
authors. You will then count the unique words and total number of words in each
of these texts. This will allow you to compute the frequency of occurrence of
each word in each of the texts. You will do the same for the anonymous sample of
text. From these frequencies, you can then attempt to identify the author of the
sample by computing the \emph{distance} of the sample to each of the known
texts. There are three distance metrics that you are considering to use:
\emph{Manhattan distance}, \emph{Euclidean distance}, and \emph{cosine
distance}.

As an example to illustrate the three metrics we will use the examples texts:
\begin{enumerate}
  \item ``Hello world''
  \item ``Goodbye, goodbye world''
\end{enumerate}
These will give us 3-dimensional vectors which correspond to:
$< \text{``hello''}, \text{``goodbye''},
\text{``world''} >$, which the texts have respectively as:
\begin{enumerate}
  \item $\vec{u'} = <1, 0, 1>$
  \item $\vec{v'} = <0, 2, 1>$
\end{enumerate}
Notice that ``hello'' and ``goodbye'' have been changed to their lowercase form and
that we ignore the punctuation.
We then want to normalize these vectors so that they each have a magnitude of 1,
giving us:
\begin{enumerate}
  \item $\vec{u} = \frac{\vec{u'}}{|\vec{u'}|} = <\frac{1}{2}, \frac{0}{2}, \frac{1}{2}>$
  \item $\vec{v} = \frac{\vec{v'}}{|\vec{v'}|} = <\frac{0}{3}, \frac{2}{3}, \frac{1}{3}>$
\end{enumerate}
We will define vectors $\vec{u}$ and $\vec{v}$ as having components
$<u_1,u_2, \ldots, u_{n-1}, u_n>$ and
$<v_1,v_2, \ldots, v_{n-1}, v_n>$,
respectively, where they are the vectors of two texts we want to compare.

\subsection{Manhattan Distance}

The \emph{Manhattan distance} is the simplest of the three to compute. To
compute it, you simply take the magnitude of the difference between each
component of the vector. You can think of it as how far you would have to go if
you had to follow the gridded streets of Manhattan. This is computed as:
\[
  \operatorname{MD} = \displaystyle\sum_{i\in n} | u_i - v_i |
\]

\noindent With the example texts we get the absolute value of the component
distances before summing them:

\begin{itemize}
  \item ``hello'' : $|u_1 - v_1| = |\frac12 - \frac03| = \frac12$
  \item ``goodbye'' : $|u_2 - v_2| = |\frac02 - \frac23| = \frac23$
  \item ``world'' : $|u_3 - v_3| = |\frac12 - \frac13| = \frac1{6}$
\end{itemize}

\noindent This gives us a Manhattan distance of $\frac43$.

\subsection{Euclidean Distance}

The \emph{Euclidean distance} is calculated very similarly to the Manhattan
distance, except you are able to go as the bird flies. Thus, we just need to
remember how Euclid taught us to find the length of the hypotenuse of a
triangle, so we add the squares of all the magnitudes together before getting
the square root of the sum. This is computed as:
\[
  \operatorname{ED} = \sqrt{\displaystyle\sum_{i\in n} (u_i - v_i)^2 }
\]

\noindent With the example texts we first square the component distances before
summing them:
\begin{itemize}
  \item ``hello'' : $(u_1 - v_1|)^2 = (\frac12 - \frac03)^2 = \frac14$
  \item ``goodbye'' : $(u_2 - v_2|)^2 = (\frac02 - \frac23)^2 = \frac49$
  \item ``world'' : $(u_3 - v_3|)^2 = (\frac12 - \frac13)^2 = \frac1{36}$
\end{itemize}

\noindent We then add all the values together to get a sum of $\frac{26}{36}$,
giving us a Euclidean distance of $\sqrt{\frac{26}{36}} \approx 0.85$.

\subsection{Cosine Distance}

The \emph{cosine distance} is different from the first two in that we derive it
from the \emph{cosine similarity} of two vectors, or the cosine of the angle
between two vectors.
\[
  \operatorname{Cos}(\Theta) = \displaystyle\frac{\vec{u}\cdot
  \vec{v}}{|\vec{u}|\times|\vec{v}|}
\]
If we are to normalize the vectors, the bottom of the fraction is just 1, and so
we only need to get the dot product of the two vectors:
\[
  \vec u\cdot \vec v = \displaystyle\sum_{i \in n} u_i \times v_i
\]

Note that the angle approaches $0$ as the cosine of the angle approaches $1$.
To stay consistent with the other metrics, we instead get the cosine
\emph{distance} by subtracting the similarity from 1.  This ensures that the
texts that are more similar will have a smaller computed distance. For our
example texts we multiply the vector components together before summing all the
results:

\begin{itemize}
  \item ``hello'' : $ u_1 \times v_1 = \frac12 \times \frac03 = 0$
  \item ``goodbye'' : $ u_2 \times v_2 = \frac02 \times\frac23 = 0$
  \item ``world'' : $ u_3 \times v_3 = \frac12\times\frac13 = \frac16$
\end{itemize}

\noindent The computed cosine similarity is $\frac16$. We subtract this from $1$
for the cosine distance, giving us a cosine distance of $1-\frac16 = \frac56$.
